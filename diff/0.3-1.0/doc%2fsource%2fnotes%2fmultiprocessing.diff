
diff --git a/doc/source/notes/multiprocessing.rst b/doc/source/notes/multiprocessing.rst
index 85b7d6a..3c3c7a5 100644
--- a/doc/source/notes/multiprocessing.rst
+++ b/doc/source/notes/multiprocessing.rst
@@ -9,9 +9,13 @@ memory and will only send a handle to another process.
 
 .. note::
 
-    When a :class:`~torch.autograd.Variable` is sent to another process, both
-    the :attr:`Variable.data` and :attr:`Variable.grad.data` are going to be
-    shared.
+    When a :class:`~torch.Tensor` is sent to another process, the
+    :class:`~torch.Tensor` data is shared. If :attr:`torch.Tensor.grad` is
+    not ``None``, it is also shared. After a :class:`~torch.Tensor` without
+    a :attr:`torch.Tensor.grad` field is sent to the other process, it
+    creates a standard process-specific ``.grad`` :class:`~torch.Tensor` that
+    is not automatically shared across all processes, unlike how the
+    :class:`~torch.Tensor`'s data has been shared.
 
 This allows to implement various training methods, like Hogwild, A3C, or any
 others that require asynchronous operation.
@@ -119,6 +123,6 @@ example below as well::
             p.start()
             processes.append(p)
         for p in processes:
-          p.join()
+            p.join()
 
 .. __: https://github.com/pytorch/examples/tree/master/mnist_hogwild
