
diff --git a/tutorial/beginner_source/data_loading_tutorial.py b/tutorial/beginner_source/data_loading_tutorial.py
index 617813c..3793e5a 100644
--- a/tutorial/beginner_source/data_loading_tutorial.py
+++ b/tutorial/beginner_source/data_loading_tutorial.py
@@ -45,10 +45,10 @@ plt.ion()   # interactive mode
 #
 # .. note::
 #     Download the dataset from `here <https://download.pytorch.org/tutorial/faces.zip>`_
-#     so that the images are in a directory named 'faces/'.
+#     so that the images are in a directory named 'data/faces/'.
 #     This dataset was actually
 #     generated by applying excellent `dlib's pose
-#     estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`__
+#     estimation <https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`__
 #     on a few images from imagenet tagged as 'face'.
 #
 # Dataset comes with a csv file with annotations which looks like this:
@@ -63,7 +63,7 @@ plt.ion()   # interactive mode
 # is the number of landmarks.
 #
 
-landmarks_frame = pd.read_csv('faces/face_landmarks.csv')
+landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')
 
 n = 65
 img_name = landmarks_frame.iloc[n, 0]
@@ -87,7 +87,7 @@ def show_landmarks(image, landmarks):
     plt.pause(0.001)  # pause a bit so that plots are updated
 
 plt.figure()
-show_landmarks(io.imread(os.path.join('faces/', img_name)),
+show_landmarks(io.imread(os.path.join('data/faces/', img_name)),
                landmarks)
 plt.show()
 
@@ -154,8 +154,8 @@ class FaceLandmarksDataset(Dataset):
 # will print the sizes of first 4 samples and show their landmarks.
 #
 
-face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',
-                                    root_dir='faces/')
+face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',
+                                    root_dir='data/faces/')
 
 fig = plt.figure()
 
@@ -179,7 +179,7 @@ for i in range(len(face_dataset)):
 # Transforms
 # ----------
 #
-# One issue we can see from the above is that the samples are not of the 
+# One issue we can see from the above is that the samples are not of the
 # same size. Most neural networks expect the images of a fixed size.
 # Therefore, we will need to write some prepocessing code.
 # Let's create three transforms:
@@ -192,7 +192,7 @@ for i in range(len(face_dataset)):
 #
 # We will write them as callable classes instead of simple functions so
 # that parameters of the transform need not be passed everytime it's
-# called. For this, we just need to implement ``__call__`` method and 
+# called. For this, we just need to implement ``__call__`` method and
 # if required, ``__init__`` method. We can then use a transform like this:
 #
 # ::
@@ -278,7 +278,7 @@ class ToTensor(object):
 
     def __call__(self, sample):
         image, landmarks = sample['image'], sample['landmarks']
- 
+
         # swap color axis because
         # numpy image: H x W x C
         # torch image: C X H X W
@@ -324,7 +324,7 @@ plt.show()
 # -----------------------------
 #
 # Let's put this all together to create a dataset with composed
-# transforms. 
+# transforms.
 # To summarize, every time this dataset is sampled:
 #
 # -  An image is read from the file on the fly
@@ -336,8 +336,8 @@ plt.show()
 # loop as before.
 #
 
-transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',
-                                           root_dir='faces/',
+transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',
+                                           root_dir='data/faces/',
                                            transform=transforms.Compose([
                                                Rescale(256),
                                                RandomCrop(224),
@@ -425,7 +425,7 @@ for i_batch, sample_batched in enumerate(dataloader):
 #
 # where 'ants', 'bees' etc. are class labels. Similarly generic transforms
 # which operate on ``PIL.Image`` like  ``RandomHorizontalFlip``, ``Scale``,
-# are also avaiable. You can use these to write a dataloader like this: ::
+# are also available. You can use these to write a dataloader like this: ::
 #
 #   import torch
 #   from torchvision import transforms, datasets
