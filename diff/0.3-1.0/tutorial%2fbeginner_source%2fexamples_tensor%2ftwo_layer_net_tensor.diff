
diff --git a/tutorial/beginner_source/examples_tensor/two_layer_net_tensor.py b/tutorial/beginner_source/examples_tensor/two_layer_net_tensor.py
index d0339a4..e3cb494 100644
--- a/tutorial/beginner_source/examples_tensor/two_layer_net_tensor.py
+++ b/tutorial/beginner_source/examples_tensor/two_layer_net_tensor.py
@@ -21,20 +21,21 @@ just cast the Tensor to a cuda datatype.
 import torch
 
 
-dtype = torch.FloatTensor
-# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU
+dtype = torch.float
+device = torch.device("cpu")
+# device = torch.device("cuda:0") # Uncomment this to run on GPU
 
 # N is batch size; D_in is input dimension;
 # H is hidden dimension; D_out is output dimension.
 N, D_in, H, D_out = 64, 1000, 100, 10
 
 # Create random input and output data
-x = torch.randn(N, D_in).type(dtype)
-y = torch.randn(N, D_out).type(dtype)
+x = torch.randn(N, D_in, device=device, dtype=dtype)
+y = torch.randn(N, D_out, device=device, dtype=dtype)
 
 # Randomly initialize weights
-w1 = torch.randn(D_in, H).type(dtype)
-w2 = torch.randn(H, D_out).type(dtype)
+w1 = torch.randn(D_in, H, device=device, dtype=dtype)
+w2 = torch.randn(H, D_out, device=device, dtype=dtype)
 
 learning_rate = 1e-6
 for t in range(500):
@@ -44,7 +45,7 @@ for t in range(500):
     y_pred = h_relu.mm(w2)
 
     # Compute and print loss
-    loss = (y_pred - y).pow(2).sum()
+    loss = (y_pred - y).pow(2).sum().item()
     print(t, loss)
 
     # Backprop to compute gradients of w1 and w2 with respect to loss
