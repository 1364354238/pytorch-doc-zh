
diff --git a/tutorial/beginner_source/nlp/deep_learning_tutorial.py b/tutorial/beginner_source/nlp/deep_learning_tutorial.py
index 5ef6ad0..49d14f6 100644
--- a/tutorial/beginner_source/nlp/deep_learning_tutorial.py
+++ b/tutorial/beginner_source/nlp/deep_learning_tutorial.py
@@ -25,7 +25,7 @@ learned here are :math:`A` and :math:`b`. Often, :math:`b` is refered to
 as the *bias* term.
 
 
-Pytorch and most other deep learning frameworks do things a little
+PyTorch and most other deep learning frameworks do things a little
 differently than traditional linear algebra. It maps the rows of the
 input instead of the columns. That is, the :math:`i`'th row of the
 output below is the mapping of the :math:`i`'th row of the input under
@@ -36,7 +36,6 @@ output below is the mapping of the :math:`i`'th row of the input under
 # Author: Robert Guthrie
 
 import torch
-import torch.autograd as autograd
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
@@ -48,7 +47,7 @@ torch.manual_seed(1)
 
 lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b
 # data is 2x5.  A maps from 5 to 3... can we map "data" under A?
-data = autograd.Variable(torch.randn(2, 5))
+data = torch.randn(2, 5)
 print(lin(data))  # yes
 
 
@@ -93,7 +92,7 @@ print(lin(data))  # yes
 # In pytorch, most non-linearities are in torch.functional (we have it imported as F)
 # Note that non-linearites typically don't have parameters like affine maps do.
 # That is, they don't have weights that are updated during training.
-data = autograd.Variable(torch.randn(2, 2))
+data = torch.randn(2, 2)
 print(data)
 print(F.relu(data))
 
@@ -121,7 +120,7 @@ print(F.relu(data))
 #
 
 # Softmax is also in torch.nn.functional
-data = autograd.Variable(torch.randn(5))
+data = torch.randn(5)
 print(data)
 print(F.softmax(data, dim=0))
 print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!
@@ -158,9 +157,9 @@ print(F.log_softmax(data, dim=0))  # theres also log_softmax
 # =========================
 #
 # So what we can compute a loss function for an instance? What do we do
-# with that? We saw earlier that autograd.Variable's know how to compute
-# gradients with respect to the things that were used to compute it. Well,
-# since our loss is an autograd.Variable, we can compute gradients with
+# with that? We saw earlier that Tensors know how to compute gradients
+# with respect to the things that were used to compute it. Well,
+# since our loss is an Tensor, we can compute gradients with
 # respect to all of the parameters used to compute it! Then we can perform
 # standard gradient updates. Let :math:`\theta` be our parameters,
 # :math:`L(\theta)` the loss function, and :math:`\eta` a positive
@@ -184,21 +183,22 @@ print(F.log_softmax(data, dim=0))  # theres also log_softmax
 
 
 ######################################################################
-# Creating Network Components in Pytorch
+# Creating Network Components in PyTorch
 # ======================================
 #
 # Before we move on to our focus on NLP, lets do an annotated example of
-# building a network in Pytorch using only affine maps and
+# building a network in PyTorch using only affine maps and
 # non-linearities. We will also see how to compute a loss function, using
-# Pytorch's built in negative log likelihood, and update parameters by
+# PyTorch's built in negative log likelihood, and update parameters by
 # backpropagation.
 #
 # All network components should inherit from nn.Module and override the
 # forward() method. That is about it, as far as the boilerplate is
 # concerned. Inheriting from nn.Module provides functionality to your
 # component. For example, it makes it keep track of its trainable
-# parameters, you can swap it between CPU and GPU with the .cuda() or
-# .cpu() functions, etc.
+# parameters, you can swap it between CPU and GPU with the ``.to(device)``
+# method, where device can be a CPU device ``torch.device("cpu")`` or CUDA
+# device ``torch.device("cuda:0")``.
 #
 # Let's write an annotated example of a network that takes in a sparse
 # bag-of-words representation and outputs a probability distribution over
@@ -211,7 +211,7 @@ print(F.log_softmax(data, dim=0))  # theres also log_softmax
 # Example: Logistic Regression Bag-of-Words classifier
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 #
-# Our model will map a sparse BOW representation to log probabilities over
+# Our model will map a sparse BoW representation to log probabilities over
 # labels. We assign each word in the vocab an index. For example, say our
 # entire vocab is two words "hello" and "world", with indices 0 and 1
 # respectively. The BoW vector for the sentence "hello hello hello hello"
@@ -297,16 +297,18 @@ model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)
 # Whenever you assign a component to a class variable in the __init__ function
 # of a module, which was done with the line
 # self.linear = nn.Linear(...)
-# Then through some Python magic from the Pytorch devs, your module
+# Then through some Python magic from the PyTorch devs, your module
 # (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters
 for param in model.parameters():
     print(param)
 
-# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable
-sample = data[0]
-bow_vector = make_bow_vector(sample[0], word_to_ix)
-log_probs = model(autograd.Variable(bow_vector))
-print(log_probs)
+# To run the model, pass in a BoW vector
+# Here we don't need to train, so the code is wrapped in torch.no_grad()
+with torch.no_grad():
+    sample = data[0]
+    bow_vector = make_bow_vector(sample[0], word_to_ix)
+    log_probs = model(bow_vector)
+    print(log_probs)
 
 
 ######################################################################
@@ -334,10 +336,11 @@ label_to_ix = {"SPANISH": 0, "ENGLISH": 1}
 #
 
 # Run on test data before we train, just to see a before-and-after
-for instance, label in test_data:
-    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))
-    log_probs = model(bow_vec)
-    print(log_probs)
+with torch.no_grad():
+    for instance, label in test_data:
+        bow_vec = make_bow_vector(instance, word_to_ix)
+        log_probs = model(bow_vec)
+        print(log_probs)
 
 # Print the matrix column corresponding to "creo"
 print(next(model.parameters())[:, word_to_ix["creo"]])
@@ -350,17 +353,17 @@ optimizer = optim.SGD(model.parameters(), lr=0.1)
 # two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.
 for epoch in range(100):
     for instance, label in data:
-        # Step 1. Remember that Pytorch accumulates gradients.
+        # Step 1. Remember that PyTorch accumulates gradients.
         # We need to clear them out before each instance
         model.zero_grad()
 
         # Step 2. Make our BOW vector and also we must wrap the target in a
-        # Variable as an integer. For example, if the target is SPANISH, then
+        # Tensor as an integer. For example, if the target is SPANISH, then
         # we wrap the integer 0. The loss function then knows that the 0th
         # element of the log probabilities is the log probability
         # corresponding to SPANISH
-        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))
-        target = autograd.Variable(make_target(label, label_to_ix))
+        bow_vec = make_bow_vector(instance, word_to_ix)
+        target = make_target(label, label_to_ix)
 
         # Step 3. Run our forward pass.
         log_probs = model(bow_vec)
@@ -371,10 +374,11 @@ for epoch in range(100):
         loss.backward()
         optimizer.step()
 
-for instance, label in test_data:
-    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))
-    log_probs = model(bow_vec)
-    print(log_probs)
+with torch.no_grad():
+    for instance, label in test_data:
+        bow_vec = make_bow_vector(instance, word_to_ix)
+        log_probs = model(bow_vec)
+        print(log_probs)
 
 # Index corresponding to Spanish goes up, English goes down!
 print(next(model.parameters())[:, word_to_ix["creo"]])
@@ -385,7 +389,7 @@ print(next(model.parameters())[:, word_to_ix["creo"]])
 # Spanish is much higher in the first example, and the log probability for
 # English is much higher in the second for the test data, as it should be.
 #
-# Now you see how to make a Pytorch component, pass some data through it
+# Now you see how to make a PyTorch component, pass some data through it
 # and do gradient updates. We are ready to dig deeper into what deep NLP
 # has to offer.
 #
