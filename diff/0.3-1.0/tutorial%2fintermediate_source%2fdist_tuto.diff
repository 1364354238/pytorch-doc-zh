
diff --git a/tutorial/intermediate_source/dist_tuto.rst b/tutorial/intermediate_source/dist_tuto.rst
index 137d3c5..36d07f1 100644
--- a/tutorial/intermediate_source/dist_tuto.rst
+++ b/tutorial/intermediate_source/dist_tuto.rst
@@ -1,6 +1,6 @@
 Writing Distributed Applications with PyTorch
 =============================================
-**Author**: `Séb Arnold <http://seba1511.com>`_
+**Author**: `Séb Arnold <https://seba1511.com>`_
 
 In this short tutorial, we will be going over the distributed package of PyTorch. We'll see how to set up the distributed setting, use the different communication strategies, and go over some the internals of the package.
 
@@ -27,7 +27,7 @@ In order to get started we need the ability to run multiple processes
 simultaneously. If you have access to compute cluster you should check
 with your local sysadmin or use your favorite coordination tool. (e.g.,
 `pdsh <https://linux.die.net/man/1/pdsh>`__,
-`clustershell <http://cea-hpc.github.io/clustershell/>`__, or
+`clustershell <https://cea-hpc.github.io/clustershell/>`__, or
 `others <https://slurm.schedmd.com/>`__) For the purpose of this
 tutorial, we will use a single machine and fork multiple processes using
 the following template.
@@ -74,7 +74,7 @@ every process will be able to coordinate through a master, using the
 same ip address and port. Note that we used the TCP backend, but we
 could have used
 `MPI <https://en.wikipedia.org/wiki/Message_Passing_Interface>`__ or
-`Gloo <http://github.com/facebookincubator/gloo>`__ instead. (c.f.
+`Gloo <https://github.com/facebookincubator/gloo>`__ instead. (c.f.
 `Section 5.1 <#communication-backends>`__) We will go over the magic
 happening in ``dist.init_process_group`` at the end of this tutorial,
 but it essentially allows processes to communicate with each other by
@@ -144,10 +144,10 @@ return a ``DistributedRequest`` object upon which we can choose to
 When using immediates we have to be careful about with our usage of the sent and received tensors.
 Since we do not know when the data will be communicated to the other process,
 we should not modify the sent tensor nor access the received tensor before ``req.wait()`` has completed.
-In other words, 
+In other words,
 
 -  writing to ``tensor`` after ``dist.isend()`` will result in undefined behaviour.
--  reading from ``tensor`` after ``dist.irecv()`` will result in undefined behaviour. 
+-  reading from ``tensor`` after ``dist.irecv()`` will result in undefined behaviour.
 
 However, after ``req.wait()``
 has been executed we are guaranteed that the communication took place,
@@ -202,7 +202,7 @@ to obtain the sum of all tensors at all processes, we can use the
     """ All-Reduce example."""
     def run(rank, size):
         """ Simple point-to-point communication. """
-        group = dist.new_group([0, 1]) 
+        group = dist.new_group([0, 1])
         tensor = torch.ones(1)
         dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)
         print('Rank ', rank, ' has data ', tensor[0])
@@ -234,6 +234,7 @@ of 6 collectives currently implemented in PyTorch.
    from all processes in ``dst``.
 -  ``dist.all_gather(tensor_list, tensor, group)``: Copies ``tensor``
    from all processes to ``tensor_list``, on all processes.
+-  ``dist.barrier(group)``: block all processes in `group` until each one has entered this function.
 
 Distributed Training
 --------------------
@@ -254,7 +255,7 @@ GitHub repository <https://github.com/seba-1511/dist_tuto.pth/>`__.
 Now that we understand how the distributed module works, let us write
 something useful with it. Our goal will be to replicate the
 functionality of
-`DistributedDataParallel <http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel>`__.
+`DistributedDataParallel <https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__.
 Of course, this will be a didactic example and in a real-world
 situtation you should use the official, well-tested and well-optimized
 version linked above.
@@ -339,26 +340,25 @@ example <https://github.com/pytorch/examples/blob/master/mnist/main.py>`__.)
 
     """ Distributed Synchronous SGD Example """
     def run(rank, size):
-            torch.manual_seed(1234)
-            train_set, bsz = partition_dataset()
-            model = Net()
-            optimizer = optim.SGD(model.parameters(),
-                                  lr=0.01, momentum=0.5)
-
-            num_batches = ceil(len(train_set.dataset) / float(bsz)) 
-            for epoch in range(10):
-                epoch_loss = 0.0
-                for data, target in train_set:
-                    data, target = Variable(data), Variable(target)
-                    optimizer.zero_grad()
-                    output = model(data)
-                    loss = F.nll_loss(output, target)
-                    epoch_loss += loss.data[0]
-                    loss.backward()
-                    average_gradients(model)
-                    optimizer.step()
-                print('Rank ', dist.get_rank(), ', epoch ',
-                      epoch, ': ', epoch_loss / num_batches) 
+        torch.manual_seed(1234)
+        train_set, bsz = partition_dataset()
+        model = Net()
+        optimizer = optim.SGD(model.parameters(),
+                              lr=0.01, momentum=0.5)
+
+        num_batches = ceil(len(train_set.dataset) / float(bsz))
+        for epoch in range(10):
+            epoch_loss = 0.0
+            for data, target in train_set:
+                optimizer.zero_grad()
+                output = model(data)
+                loss = F.nll_loss(output, target)
+                epoch_loss += loss.item()
+                loss.backward()
+                average_gradients(model)
+                optimizer.step()
+            print('Rank ', dist.get_rank(), ', epoch ',
+                  epoch, ': ', epoch_loss / num_batches)
 
 It remains to implement the ``average_gradients(model)`` function, which
 simply takes in a model and averages its gradients across the whole
@@ -371,16 +371,16 @@ world.
         size = float(dist.get_world_size())
         for param in model.parameters():
             dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)
-            param.grad.data /= size 
+            param.grad.data /= size
 
 *Et voilà*! We successfully implemented distributed synchronous SGD and
 could train any model on a large computer cluster.
 
 **Note:** While the last sentence is *technically* true, there are `a
-lot more tricks <http://seba-1511.github.io/dist_blog>`__ required to
+lot more tricks <https://seba-1511.github.io/dist_blog>`__ required to
 implement a production-level implementation of synchronous SGD. Again,
 use what `has been tested and
-optimized <http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel>`__.
+optimized <https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__.
 
 Our Own Ring-Allreduce
 ~~~~~~~~~~~~~~~~~~~~~~
@@ -422,9 +422,9 @@ slightly different signature than the ones in PyTorch. It takes a
 ``recv`` tensor and will store the sum of all ``send`` tensors in it. As
 an exercise left to the reader, there is still one difference between
 our version and the one in DeepSpeech: their implementation divide the
-gradient tensor into *chunks*, so as to optimially utilize the
+gradient tensor into *chunks*, so as to optimally utilize the
 communication bandwidth. (Hint:
-`toch.chunk <http://pytorch.org/docs/master/torch.html#torch.chunk>`__)
+`torch.chunk <https://pytorch.org/docs/stable/torch.html#torch.chunk>`__)
 
 Advanced Topics
 ---------------
@@ -447,7 +447,7 @@ there are currently three backends implemented in PyTorch: TCP, MPI, and
 Gloo. They each have different specifications and tradeoffs, depending
 on the desired use-case. A comparative table of supported functions can
 be found
-`here <http://pytorch.org/docs/master/distributed.html#module-torch.distributed>`__.
+`here <https://pytorch.org/docs/stable/distributed.html#module-torch.distributed>`__. Note that a fourth backend, NCCL, has been added since the creation of this tutorial.  See `this section <https://pytorch.org/docs/stable/distributed.html#multi-gpu-collective-functions>`__ of the ``torch.distributed`` docs for more information about its use and value.
 
 **TCP Backend**
 
@@ -480,10 +480,9 @@ modifications:
 
 0. ``init_processes(rank, size, fn, backend='tcp')`` :math:`\rightarrow`
    ``init_processes(rank, size, fn, backend='gloo')``
-1. ``model = Net()`` :math:`\rightarrow` ``model = Net().cuda(rank)``
-2. ``data, target = Variable(data), Variable(target)``
-   :math:`\rightarrow`
-   ``data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))``
+1.  Use ``device = torch.device("cuda:{}".format(rank))``
+2. ``model = Net()`` :math:`\rightarrow` ``model = Net().to(device)``
+3.  Use ``data, target = data.to(device), target.to(device)``
 
 With the above modifications, our model is now training on two GPUs and
 you can monitor their utilization with ``watch nvidia-smi``.
@@ -501,7 +500,7 @@ optimized for different purposes. The advantage of using the MPI backend
 lies in MPI's wide availability - and high-level of optimization - on
 large computer clusters. `Some <https://developer.nvidia.com/mvapich>`__
 `recent <https://developer.nvidia.com/ibm-spectrum-mpi>`__
-`implementations <http://www.open-mpi.org/>`__ are also able to take
+`implementations <https://www.open-mpi.org/>`__ are also able to take
 advantage of CUDA IPC and GPU Direct technologies in order to avoid
 memory copies through the CPU.
 
@@ -510,7 +509,7 @@ and we'll have to recompile it by hand. Fortunately, this process is
 fairly simple given that upon compilation, PyTorch will look *by itself*
 for an available MPI implementation. The following steps install the MPI
 backend, by installing PyTorch `from
-sources <https://github.com/pytorch/pytorch#from-source>`__.
+source <https://github.com/pytorch/pytorch#from-source>`__.
 
 1. Create and activate your Anaconda environment, install all the
    pre-requisites following `the
@@ -553,7 +552,7 @@ Those methods allow you to define how this coordination is done.
 Depending on your hardware setup, one of these methods should be
 naturally more suitable than the others. In addition to the following
 sections, you should also have a look at the `official
-documentation <http://pytorch.org/docs/master/distributed.html#initialization>`__.
+documentation <https://pytorch.org/docs/stable/distributed.html#initialization>`__.
 
 Before diving into the initialization methods, let's have a quick look
 at what happens behind ``init_process_group`` from the C/C++
@@ -674,7 +673,7 @@ multiple jobs to be scheduled on the same cluster.
 I'd like to thank the PyTorch developers for doing such a good job on
 their implementation, documentation, and tests. When the code was
 unclear, I could always count on the
-`docs <http://pytorch.org/docs/master/distributed.html>`__ or the
+`docs <https://pytorch.org/docs/stable/distributed.html>`__ or the
 `tests <https://github.com/pytorch/pytorch/blob/master/test/test_distributed.py>`__
 to find an answer. In particular, I'd like to thank Soumith Chintala,
 Adam Paszke, and Natalia Gimelshein for providing insightful comments
