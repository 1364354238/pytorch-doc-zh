
diff --git a/tutorial/intermediate_source/spatial_transformer_tutorial.py b/tutorial/intermediate_source/spatial_transformer_tutorial.py
index 277ec60..9fa3317 100644
--- a/tutorial/intermediate_source/spatial_transformer_tutorial.py
+++ b/tutorial/intermediate_source/spatial_transformer_tutorial.py
@@ -34,7 +34,6 @@ import torch.nn.functional as F
 import torch.optim as optim
 import torchvision
 from torchvision import datasets, transforms
-from torch.autograd import Variable
 import matplotlib.pyplot as plt
 import numpy as np
 
@@ -48,7 +47,7 @@ plt.ion()   # interactive mode
 # standard convolutional network augmented with a spatial transformer
 # network.
 
-use_cuda = torch.cuda.is_available()
+device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
 # Training dataset
 train_loader = torch.utils.data.DataLoader(
@@ -114,8 +113,8 @@ class Net(nn.Module):
         )
 
         # Initialize the weights/bias with identity transformation
-        self.fc_loc[2].weight.data.fill_(0)
-        self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])
+        self.fc_loc[2].weight.data.zero_()
+        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))
 
     # Spatial transformer network forward function
     def stn(self, x):
@@ -143,9 +142,7 @@ class Net(nn.Module):
         return F.log_softmax(x, dim=1)
 
 
-model = Net()
-if use_cuda:
-    model.cuda()
+model = Net().to(device)
 
 ######################################################################
 # Training the model
@@ -162,10 +159,8 @@ optimizer = optim.SGD(model.parameters(), lr=0.01)
 def train(epoch):
     model.train()
     for batch_idx, (data, target) in enumerate(train_loader):
-        if use_cuda:
-            data, target = data.cuda(), target.cuda()
+        data, target = data.to(device), target.to(device)
 
-        data, target = Variable(data), Variable(target)
         optimizer.zero_grad()
         output = model(data)
         loss = F.nll_loss(output, target)
@@ -174,32 +169,31 @@ def train(epoch):
         if batch_idx % 500 == 0:
             print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                 epoch, batch_idx * len(data), len(train_loader.dataset),
-                100. * batch_idx / len(train_loader), loss.data[0]))
+                100. * batch_idx / len(train_loader), loss.item()))
 #
 # A simple test procedure to measure STN the performances on MNIST.
 #
 
 
 def test():
-    model.eval()
-    test_loss = 0
-    correct = 0
-    for data, target in test_loader:
-        if use_cuda:
-            data, target = data.cuda(), target.cuda()
-        data, target = Variable(data, volatile=True), Variable(target)
-        output = model(data)
-
-        # sum up batch loss
-        test_loss += F.nll_loss(output, target, size_average=False).data[0]
-        # get the index of the max log-probability
-        pred = output.data.max(1, keepdim=True)[1]
-        correct += pred.eq(target.data.view_as(pred)).cpu().sum()
-
-    test_loss /= len(test_loader.dataset)
-    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'
-          .format(test_loss, correct, len(test_loader.dataset),
-                  100. * correct / len(test_loader.dataset)))
+    with torch.no_grad():
+        model.eval()
+        test_loss = 0
+        correct = 0
+        for data, target in test_loader:
+            data, target = data.to(device), target.to(device)
+            output = model(data)
+
+            # sum up batch loss
+            test_loss += F.nll_loss(output, target, size_average=False).item()
+            # get the index of the max log-probability
+            pred = output.max(1, keepdim=True)[1]
+            correct += pred.eq(target.view_as(pred)).sum().item()
+
+        test_loss /= len(test_loader.dataset)
+        print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'
+              .format(test_loss, correct, len(test_loader.dataset),
+                      100. * correct / len(test_loader.dataset)))
 
 ######################################################################
 # Visualizing the STN results
@@ -227,30 +221,26 @@ def convert_image_np(inp):
 
 
 def visualize_stn():
-    # Get a batch of training data
-    data, _ = next(iter(test_loader))
-    data = Variable(data, volatile=True)
-
-    if use_cuda:
-        data = data.cuda()
-
-    input_tensor = data.cpu().data
-    transformed_input_tensor = model.stn(data).cpu().data
+    with torch.no_grad():
+        # Get a batch of training data
+        data = next(iter(test_loader))[0].to(device)
 
-    in_grid = convert_image_np(
-        torchvision.utils.make_grid(input_tensor))
+        input_tensor = data.cpu()
+        transformed_input_tensor = model.stn(data).cpu()
 
-    out_grid = convert_image_np(
-        torchvision.utils.make_grid(transformed_input_tensor))
+        in_grid = convert_image_np(
+            torchvision.utils.make_grid(input_tensor))
 
-    # Plot the results side-by-side
-    f, axarr = plt.subplots(1, 2)
-    axarr[0].imshow(in_grid)
-    axarr[0].set_title('Dataset Images')
+        out_grid = convert_image_np(
+            torchvision.utils.make_grid(transformed_input_tensor))
 
-    axarr[1].imshow(out_grid)
-    axarr[1].set_title('Transformed Images')
+        # Plot the results side-by-side
+        f, axarr = plt.subplots(1, 2)
+        axarr[0].imshow(in_grid)
+        axarr[0].set_title('Dataset Images')
 
+        axarr[1].imshow(out_grid)
+        axarr[1].set_title('Transformed Images')
 
 for epoch in range(1, 20 + 1):
     train(epoch)
