
diff --git a/tutorial/beginner_source/nlp/word_embeddings_tutorial.py b/tutorial/beginner_source/nlp/word_embeddings_tutorial.py
index 7d4485f..23622f9 100644
--- a/tutorial/beginner_source/nlp/word_embeddings_tutorial.py
+++ b/tutorial/beginner_source/nlp/word_embeddings_tutorial.py
@@ -120,7 +120,7 @@ that the word embeddings will probably not be interpretable. That is,
 although with our hand-crafted vectors above we can see that
 mathematicians and physicists are similar in that they both like coffee,
 if we allow a neural network to learn the embeddings and see that both
-mathematicians and physicisits have a large value in the second
+mathematicians and physicists have a large value in the second
 dimension, it is not clear what that means. They are similar in some
 latent semantic dimension, but this probably has no interpretation to
 us.
@@ -159,7 +159,6 @@ indices are integers, not floats).
 # Author: Robert Guthrie
 
 import torch
-import torch.autograd as autograd
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
@@ -170,8 +169,8 @@ torch.manual_seed(1)
 
 word_to_ix = {"hello": 0, "world": 1}
 embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings
-lookup_tensor = torch.LongTensor([word_to_ix["hello"]])
-hello_embed = embeds(autograd.Variable(lookup_tensor))
+lookup_tensor = torch.tensor([word_to_ix["hello"]], dtype=torch.long)
+hello_embed = embeds(lookup_tensor)
 print(hello_embed)
 
 
@@ -240,13 +239,12 @@ model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)
 optimizer = optim.SGD(model.parameters(), lr=0.001)
 
 for epoch in range(10):
-    total_loss = torch.Tensor([0])
+    total_loss = 0
     for context, target in trigrams:
 
         # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words
-        # into integer indices and wrap them in variables)
-        context_idxs = [word_to_ix[w] for w in context]
-        context_var = autograd.Variable(torch.LongTensor(context_idxs))
+        # into integer indices and wrap them in tensors)
+        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)
 
         # Step 2. Recall that torch *accumulates* gradients. Before passing in a
         # new instance, you need to zero out the gradients from the old
@@ -255,18 +253,18 @@ for epoch in range(10):
 
         # Step 3. Run the forward pass, getting log probabilities over next
         # words
-        log_probs = model(context_var)
+        log_probs = model(context_idxs)
 
         # Step 4. Compute your loss function. (Again, Torch wants the target
-        # word wrapped in a variable)
-        loss = loss_function(log_probs, autograd.Variable(
-            torch.LongTensor([word_to_ix[target]])))
+        # word wrapped in a tensor)
+        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))
 
         # Step 5. Do the backward pass and update the gradient
         loss.backward()
         optimizer.step()
 
-        total_loss += loss.data
+        # Get the Python number from a 1-element Tensor by calling tensor.item()
+        total_loss += loss.item()
     losses.append(total_loss)
 print(losses)  # The loss decreased every iteration over the training data!
 
@@ -338,8 +336,7 @@ class CBOW(nn.Module):
 
 def make_context_vector(context, word_to_ix):
     idxs = [word_to_ix[w] for w in context]
-    tensor = torch.LongTensor(idxs)
-    return autograd.Variable(tensor)
+    return torch.tensor(idxs, dtype=torch.long)
 
 
 make_context_vector(data[0][0], word_to_ix)  # example
