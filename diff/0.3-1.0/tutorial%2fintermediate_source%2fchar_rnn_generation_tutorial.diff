
diff --git a/tutorial/intermediate_source/char_rnn_generation_tutorial.py b/tutorial/intermediate_source/char_rnn_generation_tutorial.py
index e9a56a0..6f040b5 100644
--- a/tutorial/intermediate_source/char_rnn_generation_tutorial.py
+++ b/tutorial/intermediate_source/char_rnn_generation_tutorial.py
@@ -42,7 +42,7 @@ as a "language model".
 I assume you have at least installed PyTorch, know Python, and
 understand Tensors:
 
--  http://pytorch.org/ For installation instructions
+-  https://pytorch.org/ For installation instructions
 -  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general
 -  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview
 -  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user
@@ -50,10 +50,10 @@ understand Tensors:
 It would also be useful to know about RNNs and how they work:
 
 -  `The Unreasonable Effectiveness of Recurrent Neural
-   Networks <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__
+   Networks <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__
    shows a bunch of real life examples
 -  `Understanding LSTM
-   Networks <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__
+   Networks <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__
    is about LSTMs specifically but also informative about RNNs in
    general
 
@@ -77,6 +77,7 @@ and end up with a dictionary ``{language: [names ...]}``.
 from __future__ import unicode_literals, print_function, division
 from io import open
 import glob
+import os
 import unicodedata
 import string
 
@@ -85,7 +86,7 @@ n_letters = len(all_letters) + 1 # Plus EOS marker
 
 def findFiles(path): return glob.glob(path)
 
-# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427
+# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427
 def unicodeToAscii(s):
     return ''.join(
         c for c in unicodedata.normalize('NFD', s)
@@ -102,13 +103,18 @@ def readLines(filename):
 category_lines = {}
 all_categories = []
 for filename in findFiles('data/names/*.txt'):
-    category = filename.split('/')[-1].split('.')[0]
+    category = os.path.splitext(os.path.basename(filename))[0]
     all_categories.append(category)
     lines = readLines(filename)
     category_lines[category] = lines
 
 n_categories = len(all_categories)
 
+if n_categories == 0:
+    raise RuntimeError('Data not found. Make sure that you downloaded data '
+        'from https://download.pytorch.org/tutorial/data.zip and extract it to '
+        'the current directory.')
+
 print('# categories:', n_categories, all_categories)
 print(unicodeToAscii("O'Néàl"))
 
@@ -141,7 +147,6 @@ print(unicodeToAscii("O'Néàl"))
 
 import torch
 import torch.nn as nn
-from torch.autograd import Variable
 
 class RNN(nn.Module):
     def __init__(self, input_size, hidden_size, output_size):
@@ -165,7 +170,7 @@ class RNN(nn.Module):
         return output, hidden
 
     def initHidden(self):
-        return Variable(torch.zeros(1, self.hidden_size))
+        return torch.zeros(1, self.hidden_size)
 
 
 ######################################################################
@@ -244,9 +249,9 @@ def targetTensor(line):
 # Make category, input, and target tensors from a random category, line pair
 def randomTrainingExample():
     category, line = randomTrainingPair()
-    category_tensor = Variable(categoryTensor(category))
-    input_line_tensor = Variable(inputTensor(line))
-    target_line_tensor = Variable(targetTensor(line))
+    category_tensor = categoryTensor(category)
+    input_line_tensor = inputTensor(line)
+    target_line_tensor = targetTensor(line)
     return category_tensor, input_line_tensor, target_line_tensor
 
 
@@ -267,22 +272,24 @@ criterion = nn.NLLLoss()
 learning_rate = 0.0005
 
 def train(category_tensor, input_line_tensor, target_line_tensor):
+    target_line_tensor.unsqueeze_(-1)
     hidden = rnn.initHidden()
 
     rnn.zero_grad()
 
     loss = 0
 
-    for i in range(input_line_tensor.size()[0]):
+    for i in range(input_line_tensor.size(0)):
         output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)
-        loss += criterion(output, target_line_tensor[i])
+        l = criterion(output, target_line_tensor[i])
+        loss += l
 
     loss.backward()
 
     for p in rnn.parameters():
         p.data.add_(-learning_rate, p.grad.data)
 
-    return output, loss.data[0] / input_line_tensor.size()[0]
+    return output, loss.item() / input_line_tensor.size(0)
 
 
 ######################################################################
@@ -374,24 +381,25 @@ max_length = 20
 
 # Sample from a category and starting letter
 def sample(category, start_letter='A'):
-    category_tensor = Variable(categoryTensor(category))
-    input = Variable(inputTensor(start_letter))
-    hidden = rnn.initHidden()
-
-    output_name = start_letter
-
-    for i in range(max_length):
-        output, hidden = rnn(category_tensor, input[0], hidden)
-        topv, topi = output.data.topk(1)
-        topi = topi[0][0]
-        if topi == n_letters - 1:
-            break
-        else:
-            letter = all_letters[topi]
-            output_name += letter
-        input = Variable(inputTensor(letter))
-
-    return output_name
+    with torch.no_grad():  # no need to track history in sampling
+        category_tensor = categoryTensor(category)
+        input = inputTensor(start_letter)
+        hidden = rnn.initHidden()
+
+        output_name = start_letter
+
+        for i in range(max_length):
+            output, hidden = rnn(category_tensor, input[0], hidden)
+            topv, topi = output.topk(1)
+            topi = topi[0][0]
+            if topi == n_letters - 1:
+                break
+            else:
+                letter = all_letters[topi]
+                output_name += letter
+            input = inputTensor(letter)
+
+        return output_name
 
 # Get multiple samples from one category and multiple starting letters
 def samples(category, start_letters='ABC'):
