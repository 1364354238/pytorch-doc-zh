
diff --git a/doc/source/autograd.rst b/doc/source/autograd.rst
index 032dd42..135cfcf 100644
--- a/doc/source/autograd.rst
+++ b/doc/source/autograd.rst
@@ -11,21 +11,19 @@ Automatic differentiation package - torch.autograd
 
 .. autofunction:: grad
 
-Variable
---------
+.. _locally-disable-grad:
 
-API compatibility
-^^^^^^^^^^^^^^^^^
+Locally disabling gradient computation
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Variable API is nearly the same as regular Tensor API (with the exception
-of a couple in-place methods, that would overwrite inputs required for
-gradient computation). In most cases Tensors can be safely replaced with
-Variables and the code will remain to work just fine. Because of this,
-we're not documenting all the operations on variables, and you should
-refer to :class:`torch.Tensor` docs for this purpose.
+.. autoclass:: no_grad
 
-In-place operations on Variables
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+.. autoclass:: enable_grad
+
+.. autoclass:: set_grad_enabled
+
+In-place operations on Tensors
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Supporting in-place operations in autograd is a hard matter, and we discourage
 their use in most cases. Autograd's aggressive buffer freeing and reuse makes
@@ -34,27 +32,58 @@ actually lower memory usage by any significant amount. Unless you're operating
 under heavy memory pressure, you might never need to use them.
 
 In-place correctness checks
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
+---------------------------
 
-All :class:`Variable` s keep track of in-place operations applied to them, and
-if the implementation detects that a variable was saved for backward in one of
+All :class:`Tensor` s keep track of in-place operations applied to them, and
+if the implementation detects that a tensor was saved for backward in one of
 the functions, but it was modified in-place afterwards, an error will be raised
 once backward pass is started. This ensures that if you're using in-place
 functions and not seeing any errors, you can be sure that the computed
 gradients are correct.
 
+Variable (deprecated)
+^^^^^^^^^^^^^^^^^^^^^
 
-.. autoclass:: Variable
-    :members:
+.. warning::
+    The Variable API has been deprecated: Variables are no longer necessary to
+    use autograd with tensors. Autograd automatically supports Tensors with
+    ``requires_grad`` set to ``True``. Below please find a quick guide on what
+    has changed:
+
+    - ``Variable(tensor)`` and ``Variable(tensor, requires_grad)`` still work as expected,
+      but they return Tensors instead of Variables.
+    - ``var.data`` is the same thing as ``tensor.data``.
+    - Methods such as ``var.backward(), var.detach(), var.register_hook()`` now work on tensors
+      with the same method names.
+
+    In addition, one can now create tensors with ``requires_grad=True`` using factory
+    methods such as :func:`torch.randn`, :func:`torch.zeros`, :func:`torch.ones`, and others
+    like the following:
+
+    ``autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)``
+
+Tensor autograd functions
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+.. autoclass:: torch.Tensor
+   :members: grad, requires_grad, is_leaf, backward, detach, detach_, register_hook, retain_grad
 
 :hidden:`Function`
-------------------
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. autoclass:: Function
     :members:
 
+.. _grad-check:
+
+Numerical gradient checking
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+.. autofunction:: gradcheck
+
+.. autofunction:: gradgradcheck
+
 Profiler
---------
+^^^^^^^^
 
 Autograd includes a profiler that lets you inspect the cost of different
 operators inside your model - both on the CPU and GPU. There are two modes
@@ -69,3 +98,10 @@ and nvprof based (registers both CPU and GPU activity) using
     :members:
 
 .. autofunction:: torch.autograd.profiler.load_nvprof
+
+Anomaly detection
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+.. autoclass:: detect_anomaly
+
+.. autoclass:: set_detect_anomaly
